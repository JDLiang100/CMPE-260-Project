{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN3MRr-l-jVJ"
      },
      "source": [
        "# Humanaoid Standup Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeaoWtRK6DWT"
      },
      "source": [
        "Enviroment Setup and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4O5fmSVs-iNQ",
        "outputId": "8387f89b-0ab9-4709-b164-204f2e427833"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Requirement already satisfied: mujoco>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (3.3.7)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (2.37.0)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (25.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.3.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.12/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.13.0)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.12/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (2.10.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.12/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2025.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.23.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[mujoco]\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.distributions import Normal\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# device setup\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "# enviroment\n",
        "env = gym.make('HumanoidStandup-v5')\n",
        "obs, info = env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jvBtfT-3xcr"
      },
      "source": [
        "Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jOUnYBak3yem"
      },
      "outputs": [],
      "source": [
        "class Memory():\n",
        "  def __init__(self, batch_size):\n",
        "    self.states = []\n",
        "    self.values = []\n",
        "    self.actions = []\n",
        "    self.probs = []\n",
        "    self.rewards = []\n",
        "    self.finished = []\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def store_transition_data(self, state, value, a, prob, reward, finished):\n",
        "    self.states.append(state)\n",
        "    if torch.is_tensor(value):\n",
        "      value = value.detach().squeeze().item()\n",
        "    self.values.append(value)\n",
        "\n",
        "    if torch.is_tensor(a):\n",
        "      a = a.detach().cpu().squeeze(0)\n",
        "    self.actions.append(a)\n",
        "\n",
        "    if torch.is_tensor(prob):\n",
        "      prob = prob.detach().cpu().squeeze().item()\n",
        "    self.probs.append(prob)\n",
        "\n",
        "    self.rewards.append(reward)\n",
        "    self.finished.append(finished)\n",
        "\n",
        "  def create_batches(self):\n",
        "    states = torch.tensor(np.array(self.states), dtype = torch.float32).to(device)\n",
        "    values = torch.tensor(self.values, dtype = torch.float32).to(device)\n",
        "    actions = torch.tensor(np.array(self.actions), dtype = torch.float32).to(device)\n",
        "    probs = torch.tensor(self.probs, dtype = torch.float32).to(device)\n",
        "    rewards = torch.tensor(self.rewards, dtype = torch.float32).to(device)\n",
        "    finished = torch.tensor(self.finished, dtype = torch.bool).to(device)\n",
        "\n",
        "    random_indices = np.random.permutation(len(states))\n",
        "    batch_starting_indices = np.arange(0,len(states), self.batch_size)\n",
        "    batches = []\n",
        "\n",
        "    for b in batch_starting_indices:\n",
        "      batches.append(random_indices[b:b+self.batch_size])\n",
        "\n",
        "    return states, values, actions, probs, rewards, finished, batches\n",
        "\n",
        "  def restart_memory(self):\n",
        "    self.states = []\n",
        "    self.values = []\n",
        "    self.actions = []\n",
        "    self.probs = []\n",
        "    self.rewards = []\n",
        "    self.finished = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1o7owIjxJfY"
      },
      "source": [
        "###Actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LuNxRcYk_5dm"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "  def __init__(self, observation_dim, action_dim, hidden_size = 256):\n",
        "    super().__init__()\n",
        "    self.actor = nn.Sequential(\n",
        "        nn.Linear(observation_dim, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, action_dim) # represents the mean\n",
        "    )\n",
        "    self.std_logged = nn.Parameter(torch.zeros(action_dim)) # trainable std parameter\n",
        "\n",
        "  def forward(self,observation):\n",
        "    mean = self.actor(observation)\n",
        "    std = torch.exp(self.std_logged)\n",
        "    return mean, std\n",
        "\n",
        "  def compute_distr(self, observation):\n",
        "    mean, std = self.forward(observation)\n",
        "    distr = torch.distributions.Normal(mean, std)\n",
        "    return distr\n",
        "\n",
        "  def take_action(self, observation):\n",
        "    distr = self.compute_distr(observation)\n",
        "    action_sampled = distr.sample()\n",
        "    log_probabilty  = distr.log_prob(action_sampled).sum(dim= -1)\n",
        "    return action_sampled, log_probabilty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uAtaRDNxEAl"
      },
      "source": [
        "###Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "urNWi4sCu1Np"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "  def __init__(self, observation, hidden_size = 256):\n",
        "    super().__init__()\n",
        "    self.critic = nn.Sequential(\n",
        "        nn.Linear(observation, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, 1) # 1 dim represents the value of the state\n",
        "    )\n",
        "  def forward(self, observation):\n",
        "    return self.critic(observation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gREmmQknxQbx"
      },
      "source": [
        "### Interacting Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OMi_4wkQxTto"
      },
      "outputs": [],
      "source": [
        "class PPOController:\n",
        "  def __init__(self,observation_dim, action_dim, lambda_, gamma, alpha,  clip, N, epochs, batch_size):\n",
        "    self.observation_dim = observation_dim\n",
        "    self.controller_memory = Memory(batch_size)\n",
        "    self.action_dim = action_dim\n",
        "    self.lambda_ = lambda_\n",
        "    self.gamma = gamma\n",
        "    self.alpha = alpha\n",
        "    self.clip = clip\n",
        "    self.N = N\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.actor = Actor(observation_dim, action_dim).to(device)\n",
        "    self.critic = Critic(observation_dim).to(device)\n",
        "    self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr = alpha)\n",
        "    self.critic_opt = torch.optim.Adam(self.critic.parameters(), lr = alpha)\n",
        "\n",
        "  def sample_action(self,observation):\n",
        "    observation_torch = torch.as_tensor(observation, dtype = torch.float32).to(device)\n",
        "    if observation_torch.dim() == 1:\n",
        "      observation_torch = observation_torch.unsqueeze(0) # add batch dim\n",
        "    action_sampled, log_probabilty = self.actor.take_action(observation_torch)\n",
        "    state_value = self.critic(observation_torch)\n",
        "    return action_sampled, log_probabilty, state_value\n",
        "\n",
        "\n",
        "  def compute_gae(self):\n",
        "    states, values, actions, prev_probs, rewards, finished, batches  = self.controller_memory.create_batches()\n",
        "    add_one = torch.zeros(1, dtype = values.dtype).to(device)\n",
        "    vals = torch.cat([values, add_one], dim = 0)\n",
        "    advantages = torch.zeros_like(rewards).to(device)\n",
        "    prev_advantage = 0\n",
        "    T = len(rewards)\n",
        "\n",
        "    # computes gae (reversed)\n",
        "    for t in reversed(range(T)):\n",
        "      finished_mask = 1 - finished[t].float()\n",
        "      td_error  = rewards[t] + self.gamma * vals[t+1] *finished_mask - vals[t]\n",
        "      advantage = td_error + self.gamma * self.lambda_ * prev_advantage * finished_mask\n",
        "      prev_advantage = advantage\n",
        "      advantages[t]  = advantage\n",
        "\n",
        "    return states, values, actions, prev_probs, rewards, finished, batches, advantages\n",
        "\n",
        "\n",
        "  def iterate_batches(self):\n",
        "    states, values, actions, prev_probs, rewards, finished, batches, advantages = self.compute_gae()\n",
        "    batch_loss = []\n",
        "    for batch in batches:\n",
        "      batch_states = states[batch]\n",
        "      batch_actions = actions[batch]\n",
        "      batch_prev_probs = prev_probs[batch]\n",
        "      batch_advantages = advantages[batch]\n",
        "      batch_rewards = rewards[batch]\n",
        "      batch_values = values[batch]\n",
        "\n",
        "      distribution = self.actor.compute_distr(batch_states)\n",
        "      new_log_probabilty = distribution.log_prob(batch_actions).sum(dim = -1)\n",
        "      prob_ratio = (new_log_probabilty - batch_prev_probs).exp()\n",
        "\n",
        "      value_pred = self.critic(batch_states).squeeze(-1)\n",
        "\n",
        "      # computing actor loss\n",
        "      term1 = prob_ratio * batch_advantages\n",
        "      term2 = torch.clamp(prob_ratio, 1-self.clip, 1+ self.clip)*batch_advantages\n",
        "      actor_loss =  -torch.min(term1, term2).mean()\n",
        "\n",
        "      # computing critic loss\n",
        "      return_ = batch_advantages + batch_values\n",
        "      critic_loss = 0.5 * ((value_pred - return_)**2).mean()\n",
        "\n",
        "      total_loss = actor_loss + critic_loss\n",
        "      batch_loss.append(total_loss)\n",
        "\n",
        "    batch_loss = torch.stack(batch_loss).mean()\n",
        "\n",
        "    return batch_loss\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "    for ep in range(self.epochs):\n",
        "      total_loss = self.iterate_batches()\n",
        "      self.actor_opt.zero_grad()\n",
        "      self.critic_opt.zero_grad()\n",
        "      total_loss.backward()\n",
        "      self.actor_opt.step()\n",
        "      self.critic_opt.step()\n",
        "    self.controller_memory.restart_memory()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pioz65vsZkqM"
      },
      "source": [
        "# Running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RDJKjrzXZkMh"
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "hidden_size = 256\n",
        "observation_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "alpha = 3e-4\n",
        "batch_size = 64\n",
        "gamma = 0.9\n",
        "lambda_ = 0.95\n",
        "clip = 0.2\n",
        "epochs = 4\n",
        "runs = 1000\n",
        "done= False\n",
        "scores = []\n",
        "mean_score = 0.0\n",
        "top_score = float('-inf')\n",
        "steps = 0\n",
        "N = 2048\n",
        "mean_scores_list = []\n",
        "\n",
        "PPOAgent = PPOController(observation_dim, action_dim, lambda_, gamma, alpha,  clip, N, epochs, batch_size)\n",
        "\n",
        "for i in range(runs):\n",
        "  observation, info = env.reset()\n",
        "  score = 0\n",
        "  done = False\n",
        "  while done == False:\n",
        "    action, log_prob, val = PPOAgent.sample_action(observation)\n",
        "\n",
        "    a = action.squeeze(0).detach().cpu().numpy()\n",
        "    obs, r, terminated, truncated, info = env.step(a)\n",
        "    done = terminated or truncated\n",
        "    steps += 1\n",
        "\n",
        "    PPOAgent.controller_memory.store_transition_data(observation, val, action, log_prob, r, done)\n",
        "    if steps % N == 0:\n",
        "      PPOAgent.train()\n",
        "    score += r\n",
        "    observation = obs\n",
        "\n",
        "  scores.append(score)\n",
        "  mean_score = np.mean(scores[-100:]) # prev 100 runs\n",
        "  mean_scores_list.append(mean_score)\n",
        "\n",
        "  if mean_score > top_score:\n",
        "    top_score = mean_score\n",
        "\n",
        "  #print(f'EP: {i}, BEST SCORE: {top_score}, AVG 50: {mean_score}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99YxIMZg2NI5"
      },
      "source": [
        "###Saving models/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Yzrry1UK0CUC"
      },
      "outputs": [],
      "source": [
        "torch.save(PPOAgent, \"PPOAgent.pth\")\n",
        "np.save('Scores.npy',np.array(scores))\n",
        "np.save('MeanScores.npy',np.array(mean_scores_list))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
