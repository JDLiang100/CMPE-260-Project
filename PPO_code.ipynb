{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN3MRr-l-jVJ"
      },
      "source": [
        "# Humanoid Standup Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeaoWtRK6DWT"
      },
      "source": [
        "Enviroment Setup and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4O5fmSVs-iNQ",
        "outputId": "431fd4a9-9414-4fee-e1ce-59c551a9b2a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Requirement already satisfied: mujoco>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (3.3.7)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (2.37.2)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (25.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.3.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.12/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.13.0)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.12/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (2.10.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.12/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2025.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.23.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[mujoco]\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.distributions import Normal\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# device setup\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jvBtfT-3xcr"
      },
      "source": [
        "Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "jOUnYBak3yem"
      },
      "outputs": [],
      "source": [
        "class Memory():\n",
        "  def __init__(self, batch_size):\n",
        "    self.states = []\n",
        "    self.values = []\n",
        "    self.actions = []\n",
        "    self.probs = []\n",
        "    self.rewards = []\n",
        "    self.finished = []\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def store_transition_data(self, state, value, a, prob, reward, finished):\n",
        "    self.states.append(state)\n",
        "    if torch.is_tensor(value):\n",
        "      value = value.detach().squeeze().item()\n",
        "    self.values.append(value)\n",
        "\n",
        "    if torch.is_tensor(a):\n",
        "      a = a.detach().cpu().squeeze(0)\n",
        "    self.actions.append(a)\n",
        "\n",
        "    if torch.is_tensor(prob):\n",
        "      prob = prob.detach().cpu().squeeze().item()\n",
        "    self.probs.append(prob)\n",
        "\n",
        "    self.rewards.append(reward)\n",
        "    self.finished.append(finished)\n",
        "\n",
        "  def create_batches(self):\n",
        "    states = torch.tensor(np.array(self.states), dtype = torch.float32).to(device)\n",
        "    values = torch.tensor(self.values, dtype = torch.float32).to(device)\n",
        "    actions = torch.tensor(np.array(self.actions), dtype = torch.float32).to(device)\n",
        "    probs = torch.tensor(self.probs, dtype = torch.float32).to(device)\n",
        "    rewards = torch.tensor(self.rewards, dtype = torch.float32).to(device)\n",
        "    finished = torch.tensor(self.finished, dtype = torch.bool).to(device)\n",
        "\n",
        "    random_indices = np.random.permutation(len(states))\n",
        "    batch_starting_indices = np.arange(0,len(states), self.batch_size)\n",
        "    batches = []\n",
        "\n",
        "    for b in batch_starting_indices:\n",
        "      batches.append(random_indices[b:b+self.batch_size])\n",
        "\n",
        "    return states, values, actions, probs, rewards, finished, batches\n",
        "\n",
        "  def restart_memory(self):\n",
        "    self.states = []\n",
        "    self.values = []\n",
        "    self.actions = []\n",
        "    self.probs = []\n",
        "    self.rewards = []\n",
        "    self.finished = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1o7owIjxJfY"
      },
      "source": [
        "###Actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "LuNxRcYk_5dm"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "  def __init__(self, observation_dim, action_dim, hidden_size = 256):\n",
        "    super().__init__()\n",
        "    self.actor = nn.Sequential(\n",
        "        nn.Linear(observation_dim, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, action_dim) # represents the mean\n",
        "    )\n",
        "    self.std_logged = nn.Parameter(torch.zeros(action_dim)) # trainable std parameter\n",
        "\n",
        "  def forward(self,observation):\n",
        "    mean = self.actor(observation)\n",
        "    std = torch.exp(self.std_logged)\n",
        "    return mean, std\n",
        "\n",
        "  def compute_distr(self, observation):\n",
        "    mean, std = self.forward(observation)\n",
        "    distr = torch.distributions.Normal(mean, std)\n",
        "    return distr\n",
        "\n",
        "  def take_action(self, observation):\n",
        "    distr = self.compute_distr(observation)\n",
        "    action_sampled = distr.sample()\n",
        "    log_probabilty  = distr.log_prob(action_sampled).sum(dim= -1)\n",
        "    return action_sampled, log_probabilty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uAtaRDNxEAl"
      },
      "source": [
        "###Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "urNWi4sCu1Np"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "  def __init__(self, observation, hidden_size = 256):\n",
        "    super().__init__()\n",
        "    self.critic = nn.Sequential(\n",
        "        nn.Linear(observation, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, 1) # 1 dim represents the value of the state\n",
        "    )\n",
        "  def forward(self, observation):\n",
        "    return self.critic(observation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gREmmQknxQbx"
      },
      "source": [
        "### Interacting Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "OMi_4wkQxTto"
      },
      "outputs": [],
      "source": [
        "class PPOController:\n",
        "  def __init__(self,observation_dim, action_dim, lambda_, gamma, alpha,  clip, N, epochs, batch_size, ent_coef = 0.01):\n",
        "    self.observation_dim = observation_dim\n",
        "    self.controller_memory = Memory(batch_size)\n",
        "    self.action_dim = action_dim\n",
        "    self.lambda_ = lambda_\n",
        "    self.gamma = gamma\n",
        "    self.alpha = alpha\n",
        "    self.clip = clip\n",
        "    self.N = N\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.actor = Actor(observation_dim, action_dim).to(device)\n",
        "    self.critic = Critic(observation_dim).to(device)\n",
        "    self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr = alpha)\n",
        "    self.critic_opt = torch.optim.Adam(self.critic.parameters(), lr = alpha)\n",
        "    self.ent_coef = ent_coef\n",
        "\n",
        "  def sample_action(self,observation):\n",
        "    observation_torch = torch.as_tensor(observation, dtype = torch.float32).to(device)\n",
        "    if observation_torch.dim() == 1:\n",
        "      observation_torch = observation_torch.unsqueeze(0) # add batch dim\n",
        "    action_sampled, log_probabilty = self.actor.take_action(observation_torch)\n",
        "    state_value = self.critic(observation_torch)\n",
        "    return action_sampled, log_probabilty, state_value\n",
        "\n",
        "\n",
        "\n",
        "  def compute_gae(self, last_value = None):\n",
        "    states, values, actions, prev_probs, rewards, finished, batches  = self.controller_memory.create_batches()\n",
        "    if last_value is None:\n",
        "      last_value_tensor = torch.zeros(1, dtype = values.dtype, device = device)\n",
        "    else:\n",
        "      if torch.is_tensor(last_value):\n",
        "        last_value_tensor = last_value.detach().view(1).to(device)\n",
        "      else:\n",
        "        last_value_tensor = torch.tensor([last_value], dtype = values.dtype, device = device)\n",
        "\n",
        "\n",
        "    vals = torch.cat([values, last_value_tensor], dim = 0)\n",
        "    advantages = torch.zeros_like(rewards).to(device)\n",
        "    prev_advantage = 0.0\n",
        "    T = len(rewards)\n",
        "\n",
        "    # computes gae (reversed)\n",
        "    for t in reversed(range(T)):\n",
        "      finished_mask = 1.0 - finished[t].float()\n",
        "      td_error  = rewards[t] + self.gamma * vals[t+1] *finished_mask - vals[t]\n",
        "      advantage = td_error + self.gamma * self.lambda_ * prev_advantage * finished_mask\n",
        "      prev_advantage = advantage\n",
        "      advantages[t]  = advantage\n",
        "\n",
        "    return states, values, actions, prev_probs, rewards, finished, batches, advantages\n",
        "\n",
        "\n",
        "\n",
        "  def iterate_batches(self, last_value = None):\n",
        "    states, values, actions, prev_probs, rewards, finished, batches, advantages = self.compute_gae(last_value)\n",
        "    returns = advantages + values\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "    batch_loss = []\n",
        "    for batch in batches:\n",
        "      batch_states = states[batch]\n",
        "      batch_actions = actions[batch]\n",
        "      batch_prev_probs = prev_probs[batch]\n",
        "      batch_advantages = advantages[batch]\n",
        "      batch_rewards = rewards[batch]\n",
        "      batch_returns = returns[batch]\n",
        "\n",
        "      distribution = self.actor.compute_distr(batch_states)\n",
        "      new_log_probabilty = distribution.log_prob(batch_actions).sum(dim = -1)\n",
        "      prob_ratio = (new_log_probabilty - batch_prev_probs).exp()\n",
        "\n",
        "      value_pred = self.critic(batch_states).squeeze(-1)\n",
        "\n",
        "      # computing actor loss\n",
        "      term1 = prob_ratio * batch_advantages\n",
        "      term2 = torch.clamp(prob_ratio, 1-self.clip, 1+ self.clip)*batch_advantages\n",
        "      actor_loss =  -torch.min(term1, term2).mean()\n",
        "\n",
        "      entropy = distribution.entropy().sum(-1).mean()\n",
        "\n",
        "\n",
        "      # computing critic loss\n",
        "      return_ = batch_returns\n",
        "      critic_loss = 0.5 * ((value_pred - return_)**2).mean()\n",
        "\n",
        "      total_loss = actor_loss + critic_loss - self.ent_coef * entropy\n",
        "      batch_loss.append(total_loss)\n",
        "\n",
        "    batch_loss = torch.stack(batch_loss).mean()\n",
        "\n",
        "    return batch_loss\n",
        "\n",
        "\n",
        "  def train(self, last_value = None):\n",
        "    for ep in range(self.epochs):\n",
        "      total_loss = self.iterate_batches(last_value)\n",
        "      self.actor_opt.zero_grad()\n",
        "      self.critic_opt.zero_grad()\n",
        "      total_loss.backward()\n",
        "      self.actor_opt.step()\n",
        "      self.critic_opt.step()\n",
        "    self.controller_memory.restart_memory()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pioz65vsZkqM"
      },
      "source": [
        "# Running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDJKjrzXZkMh",
        "outputId": "c8a8ce0d-12da-416e-fe99-81e922d6cce9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP: 0, BEST SCORE: 35843.47715060286, AVG 100: 35843.47715060286\n",
            "EP: 50, BEST SCORE: 49768.86361556381, AVG 100: 49768.86361556381\n",
            "EP: 100, BEST SCORE: 51067.12129006974, AVG 100: 50661.47814757825\n",
            "EP: 150, BEST SCORE: 51422.13540758381, AVG 100: 50885.04611256516\n",
            "EP: 200, BEST SCORE: 51422.13540758381, AVG 100: 50981.65526969561\n",
            "EP: 250, BEST SCORE: 52083.61353625544, AVG 100: 51630.155966836115\n",
            "EP: 300, BEST SCORE: 52083.61353625544, AVG 100: 51271.83670743125\n",
            "EP: 350, BEST SCORE: 52083.61353625544, AVG 100: 51210.40699296785\n",
            "EP: 400, BEST SCORE: 52083.61353625544, AVG 100: 51522.9688316711\n",
            "EP: 450, BEST SCORE: 53043.28905707785, AVG 100: 53043.28905707785\n",
            "EP: 500, BEST SCORE: 54980.57596583747, AVG 100: 54980.57596583747\n",
            "EP: 550, BEST SCORE: 55423.25422180104, AVG 100: 55369.4945887741\n",
            "EP: 600, BEST SCORE: 55914.95989915313, AVG 100: 55424.373419228854\n",
            "EP: 650, BEST SCORE: 56878.07330474533, AVG 100: 56662.87058388232\n",
            "EP: 700, BEST SCORE: 59273.424416691574, AVG 100: 59273.424416691574\n",
            "EP: 750, BEST SCORE: 63262.90765681476, AVG 100: 63262.90765681476\n",
            "EP: 800, BEST SCORE: 63934.722401865496, AVG 100: 63279.538219746\n",
            "EP: 850, BEST SCORE: 63934.722401865496, AVG 100: 61538.292858945395\n",
            "EP: 900, BEST SCORE: 63934.722401865496, AVG 100: 62636.18987101007\n",
            "EP: 950, BEST SCORE: 64418.27023052278, AVG 100: 64181.2199954929\n",
            "EP: 1000, BEST SCORE: 65388.98782245342, AVG 100: 65388.98782245342\n",
            "EP: 1050, BEST SCORE: 67253.70872274277, AVG 100: 67253.70872274277\n",
            "EP: 1100, BEST SCORE: 68074.25000318824, AVG 100: 66120.51950134378\n",
            "EP: 1150, BEST SCORE: 68074.25000318824, AVG 100: 63939.84405628768\n",
            "EP: 1200, BEST SCORE: 68074.25000318824, AVG 100: 64299.44626142745\n",
            "EP: 1250, BEST SCORE: 68074.25000318824, AVG 100: 64898.558455951315\n",
            "EP: 1300, BEST SCORE: 68074.25000318824, AVG 100: 66606.59392992235\n",
            "EP: 1350, BEST SCORE: 68535.89886881631, AVG 100: 68229.64134855852\n",
            "EP: 1400, BEST SCORE: 68618.96211176421, AVG 100: 67154.49496735778\n",
            "EP: 1450, BEST SCORE: 68618.96211176421, AVG 100: 66678.2167574651\n",
            "EP: 1500, BEST SCORE: 68629.12631691262, AVG 100: 68629.12631691262\n",
            "EP: 1550, BEST SCORE: 70627.70838756542, AVG 100: 70617.30558036007\n",
            "EP: 1600, BEST SCORE: 72952.20458088834, AVG 100: 72666.85724130756\n",
            "EP: 1650, BEST SCORE: 74377.4341377494, AVG 100: 73470.790623901\n",
            "EP: 1700, BEST SCORE: 74377.4341377494, AVG 100: 73499.25760559105\n",
            "EP: 1750, BEST SCORE: 74377.4341377494, AVG 100: 72467.26281818932\n",
            "EP: 1800, BEST SCORE: 74377.4341377494, AVG 100: 73167.50803218322\n",
            "EP: 1850, BEST SCORE: 74377.4341377494, AVG 100: 74182.47252060728\n",
            "EP: 1900, BEST SCORE: 76543.24462909959, AVG 100: 76543.24462909959\n",
            "EP: 1950, BEST SCORE: 77811.73637610165, AVG 100: 77640.1436929442\n",
            "EP: 2000, BEST SCORE: 77811.73637610165, AVG 100: 75639.09366855517\n",
            "EP: 2050, BEST SCORE: 77811.73637610165, AVG 100: 74122.14905282218\n",
            "EP: 2100, BEST SCORE: 77811.73637610165, AVG 100: 72152.72079159731\n",
            "EP: 2150, BEST SCORE: 77811.73637610165, AVG 100: 75497.58108108914\n",
            "EP: 2200, BEST SCORE: 83188.5574463601, AVG 100: 83188.5574463601\n",
            "EP: 2250, BEST SCORE: 85673.57387140047, AVG 100: 84463.15496383782\n",
            "EP: 2300, BEST SCORE: 85673.57387140047, AVG 100: 85017.38715142384\n",
            "EP: 2350, BEST SCORE: 86532.33309563587, AVG 100: 86422.3122601406\n",
            "EP: 2400, BEST SCORE: 88759.61310609357, AVG 100: 88759.61310609357\n",
            "EP: 2450, BEST SCORE: 92500.31893069608, AVG 100: 92467.0652826725\n",
            "EP: 2500, BEST SCORE: 93650.66641340432, AVG 100: 90729.23825575423\n",
            "EP: 2550, BEST SCORE: 93650.66641340432, AVG 100: 85530.27255048505\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "SEED = 24\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# enviroment\n",
        "env = gym.make('HumanoidStandup-v5', uph_cost_weight = 1.5)\n",
        "obs, info = env.reset(seed = SEED)\n",
        "\n",
        "# parameters\n",
        "observation_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "alpha = 3e-4\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "lambda_ = 0.9\n",
        "clip = 0.2\n",
        "epochs = 10\n",
        "runs = 4000\n",
        "done= False\n",
        "scores = []\n",
        "mean_score = 0.0\n",
        "top_score = float('-inf')\n",
        "steps = 0\n",
        "N = 512\n",
        "mean_scores_list = []\n",
        "entropy_coef = 0.01\n",
        "hidden_size =256\n",
        "\n",
        "PPOAgent = PPOController(observation_dim, action_dim, lambda_, gamma, alpha,  clip, N, epochs, batch_size, entropy_coef)\n",
        "\n",
        "for i in range(runs):\n",
        "  observation, info = env.reset()\n",
        "  score = 0\n",
        "  done = False\n",
        "  while done == False:\n",
        "    with torch.no_grad():\n",
        "      action, log_prob, val = PPOAgent.sample_action(observation)\n",
        "\n",
        "    unclipped_a = action.squeeze(0).cpu().numpy()\n",
        "    env_action = np.clip(unclipped_a, env.action_space.low, env.action_space.high)\n",
        "    obs, r, terminated, truncated, info = env.step(env_action)\n",
        "    done = terminated or truncated\n",
        "    steps += 1\n",
        "\n",
        "    PPOAgent.controller_memory.store_transition_data(observation, val, unclipped_a, log_prob, r, done)\n",
        "    if steps % N == 0:\n",
        "      with torch.no_grad():\n",
        "        obs_t = torch.as_tensor(obs, dtype = torch.float32).to(device)\n",
        "        last_value = PPOAgent.critic(obs_t.unsqueeze(0)).squeeze(-1)\n",
        "      PPOAgent.train(last_value)\n",
        "    score += r\n",
        "    observation = obs\n",
        "\n",
        "  scores.append(score)\n",
        "  mean_score = np.mean(scores[-100:]) # prev 100 runs\n",
        "  mean_scores_list.append(mean_score)\n",
        "\n",
        "  if mean_score > top_score:\n",
        "    top_score = mean_score\n",
        "\n",
        "  if i % 50 ==0 :\n",
        "    print(f'EP: {i}, BEST SCORE: {top_score}, AVG 100: {mean_score}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lc_EFmU1ofNa"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    f'hidden_size: {hidden_size}\\n'\n",
        "    f'observation_dim: {observation_dim}\\n'\n",
        "    f'action_dim: {action_dim}\\n'\n",
        "    f'alpha: {alpha}\\n'\n",
        "    f'batch_size: {batch_size}\\n'\n",
        "    f'gamma: {gamma}\\n'\n",
        "    f'lambda_: {lambda_}\\n'\n",
        "    f'clip: {clip}\\n'\n",
        "    f'epochs: {epochs}\\n'\n",
        "    f'runs: {runs}\\n'\n",
        "    f'N steps: {N}\\n'\n",
        "    f'entropy_coef: {entropy_coef}\\n'\n",
        "    f'lr: {alpha}\\n' )\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(10,5))\n",
        "ep = np.arange(len(scores))\n",
        "\n",
        "ax.plot(ep, scores, alpha = 0.3, label = 'Episode return')\n",
        "ax.plot(ep, mean_scores_list, label = '100-episode MA')\n",
        "ax.set_xlabel('Episode')\n",
        "ax.set_ylabel('Episode Return')\n",
        "ax.set_title('PPO HumanoidStandup')\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
